{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "# from scipy.misc import imread, imresize # read image from disc -- bring image to common shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('../DATA Human Gestures/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('../DATA Human Gestures/val.csv').readlines())\n",
    "# experimentation with batch size\n",
    "batch_size = 256 # memory efficient and maximum dat to fee din network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "* The overall structure of the generator has been given. \n",
    "* An interesting thing to note here is the use of the infinite while loop. \n",
    "    * It is there in place so that the generator is always ready to yield a batch once next() is called once it is called at the start of training. \n",
    "    * Even after one pass over the data is completed (after the for loop is completed and the batch for the remainder datapoints is yielded), upon the subsequent next() call (at the start of the next epoch), the processing starts from the command 't=np.random.permuatation(folder_list)'. \n",
    "    * In this way, the generator requires very less memory while training.\n",
    "\n",
    "\n",
    "* In the generator, \n",
    "    - Preprocess the images as you have images of 2 different dimensions,\n",
    "    - as well as create a batch of video frames.\n",
    "\n",
    "* Experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3362442636.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\sajal\\AppData\\Local\\Temp\\ipykernel_12204\\3362442636.py\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    batch_data[folder,idx,:,:,0] = #normalise and feed in the image\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generator(source_path, folder_list):\n",
    "    '''\n",
    "    source_path: root directory\n",
    "    folder_list: document\n",
    "    batch_size: data size in each batch\n",
    "    '''\n",
    "    \n",
    "    global batch_size\n",
    "    \n",
    "    print( 'Source path = ', source_path, ' ; batch size =', batch_size)\n",
    "    \n",
    "    # experiment with creating a list of image numbers you want to use for a particular video, \n",
    "    # out of all frame images in each folder -- improve performance\n",
    "    # list of index of images\n",
    "    img_idx = [1,2,5,4,8,7]\n",
    "    \n",
    "    # continuos flow of data -- for several epochs\n",
    "    while True:\n",
    "        \n",
    "        # randomly select from sequence\n",
    "        t = np.random.permutation(folder_list)\n",
    "        \n",
    "        # 1 sequence = 1 video = 30 image collection\n",
    "        # calculate the number of batches to call in each iterations -- (number of video frames)/(batch size) -- 30/batch_size\n",
    "        num_batches = 6\n",
    "        \n",
    "        # we iterate over the number of batches\n",
    "        for batch in range(num_batches):     \n",
    "            # creating 0 tensors -- empty containers -- load images\n",
    "            # x is the number of images you use for each video = img_idx\n",
    "            # (y,z) is the final size of the input images and  = height,width\n",
    "            # 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            # batch_labels is the one hot ecnoding representation of the output -- 5 classes\n",
    "            batch_labels = np.zeros((batch_size,5)) \n",
    "            \n",
    "            \n",
    "            # iterate over the batch_size\n",
    "            for folder in range(batch_size): \n",
    "                # read all the images in the video image sequence folder\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "                \n",
    "                # Iterate iver the frames/images of a folder to read them in\n",
    "                for idx,item in enumerate(img_idx): \n",
    "                    # selectively take images\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    # crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    # and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    # normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,0] =   # red\n",
    "                    batch_data[folder,idx,:,:,1] =   # green\n",
    "                    batch_data[folder,idx,:,:,2] =   # blue\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield batch_data, batch_labels \n",
    "            \n",
    "            # write the code for the remaining data points which are left after full batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sequences = 663\n",
      "validation sequences = 100\n",
      "epochs = 10\n"
     ]
    }
   ],
   "source": [
    "# current datetime\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "# paths\n",
    "train_path = '../DATA Human Gestures/train'\n",
    "val_path = '../DATA Human Gestures/val'\n",
    "\n",
    "# all training sequences\n",
    "num_train_sequences = len(train_doc)\n",
    "print('training sequences =', num_train_sequences)\n",
    "\n",
    "# all validation sequences\n",
    "num_val_sequences = len(val_doc)\n",
    "print('validation sequences =', num_val_sequences)\n",
    "\n",
    "# choose the number of epochs\n",
    "num_epochs = 10\n",
    "print ('epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. \n",
    "* Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. \n",
    "* Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.\n",
    "\n",
    " - Start from small model and small amount of data, extend model layers and data size.\n",
    " - Use high batch size and gradually reduce batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11908\\1900285988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#write your optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimiser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SGD'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# write your optimizer, loss fnction\n",
    "optimiser = 'SGD'\n",
    "loss_function = 'categorical_crossentropy'\n",
    "metrics_list = ['categorical_accuracy']\n",
    "model.compile(\n",
    "    # learning optimiser\n",
    "    optimizer=optimiser, \n",
    "    # loss function\n",
    "    loss=loss_function, \n",
    "    # metrics list\n",
    "    metrics=metrics_list,\n",
    ")\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training batch\n",
    "train_generator = generator(train_path, train_doc)\n",
    "# validaitn batch\n",
    "val_generator = generator(val_path, val_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model -- learning weights with every epoch -- current time\n",
    "model_name = 'model_gesture' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "    \n",
    "# save model detailsin filename at every epoch in disc memory-- epoch,loss,accuracy\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# automatically save model after every epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=False, \n",
    "    save_weights_only=False, mode='auto', period=1\n",
    ")\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "'''\n",
    "1. if validation loss not changing, \n",
    "2. we have hit a plateau and not changing.\n",
    "3. reduce learning rate to move towards minima and doesn't change much\n",
    "\n",
    "'''\n",
    "LR = \n",
    "# pass as a lit to fit.generator, check at every epoch, if validation loss is not decreasing, reduce learning rate\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_train_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11908\\1199428833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m '''\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_train_sequences\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_sequences\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_train_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# number of next calls fit_generator will make\n",
    "'''\n",
    "num_train_sequences,num_val_sequences = number of videos in training & validation data\n",
    "if training_sequence is divisible by batch size, take the division else floor-diviison+1\n",
    "'''\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    # pass generator object -- calls training and validation data\n",
    "    train_generator, \n",
    "    # number of batch calls per epoch\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    # number of epoxhs\n",
    "    epochs=num_epochs, \n",
    "    # book keeping for model performance with every epoch\n",
    "    verbose=1, \n",
    "    # check at avery epochs\n",
    "    callbacks=callbacks_list, \n",
    "    # test model til the epoch with validatindata\n",
    "    validation_data=val_generator, \n",
    "    # number of calls per epoch for validatin data\n",
    "    validation_steps=validation_steps, \n",
    "    # if unbalanced data is present\n",
    "    class_weight=None, \n",
    "    # fetching and computing happens at same time -- parallel computing -- \n",
    "    '''cpu fetch data -- gpu computes model on previous fetched data -- fit.generator'''\n",
    "    # \n",
    "    workers=1, \n",
    "    # initialize \n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
