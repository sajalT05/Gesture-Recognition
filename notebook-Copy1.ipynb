{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "011c5106-2667-44fd-bedf-5dc931847bbb",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "**Develop a feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.**\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "* **Thumbs up**:  Increase the volume\n",
    "* **Thumbs down**: Decrease the volume\n",
    "* **Left swipe**: 'Jump' backwards 10 seconds\n",
    "* **Right swipe**: 'Jump' forward 10 seconds  \n",
    "* **Stop**: Pause the movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9d633-8705-45c4-9b99-3093f468694b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "**Generator**: The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
    "\n",
    "**Model**: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further.\n",
    "\n",
    "**Write up**: This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db413950-cbb4-4f4c-81a7-36a4fb45a954",
   "metadata": {},
   "source": [
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aee66f1-c0d6-4875-93ac-286636a6d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import os\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efba2e49-d448-45f9-9a13-77fd2ff6a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the random seed so that the results don't vary drastically.\n",
    "\n",
    "np.random.seed(30)\n",
    "\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d10b9b8-4b70-4470-bbc7-da01929ad048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras api\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Bidirectional, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D, GlobalAveragePooling3D, ConvLSTM2D\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b19608-5b5a-4025-8144-e8de06384dc8",
   "metadata": {},
   "source": [
    "Read the folder names for training and validation:\n",
    "* Set the batch_size here, in such a way that you are able to use the GPU in full capacity. \n",
    "* Keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b10d5-4844-45ac-bb11-dde6ed19fce1",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "1. Select training and validation data for ablaion experiment.\n",
    "\n",
    "* Memory and Computation Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7f3f0c-bb9a-4a7f-9195-64dd26dd8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validatiaon data creator with ablation -- select random files for each class\n",
    "'''\n",
    "e.g. : when ablation is 10, then 10 folders for each label will be selected.\n",
    "'''\n",
    "def get_data(path, ablation=None):\n",
    "    \n",
    "    # trainining folder directory document\n",
    "    train_doc = np.random.permutation(open(path+'train.csv').readlines())\n",
    "    # validation folder directory document\n",
    "    val_doc = np.random.permutation(open(path+'val.csv').readlines())\n",
    "    \n",
    "    # count for  for 5 classes(classification) -- class label occurence in train folder \n",
    "    # -- class labels = 0,1,2,3,4\n",
    "    # -- class labels counts = [0,0,0,0,0] -- indices match with class labels\n",
    "    train_counts = np.zeros(5) \n",
    "    \n",
    "    # training data\n",
    "    train_data = []\n",
    "    # valdiatin data\n",
    "    val_data = []\n",
    "    \n",
    "    # when ablation is not None and a limit value is provided, select data till ablation limit\n",
    "    if ablation is not None:\n",
    "        # iterating train doc -- path of training data folders\n",
    "        for doc in train_doc:\n",
    "            # labels of class in folder -- [0,1,2,3,4]\n",
    "            label = int(doc.strip().split(';')[2])\n",
    "            # select folders from training data till ablation limit\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            if train_counts[label] < ablation:\n",
    "                # add folder path to training document\n",
    "                train_data.append(doc)\n",
    "                # increase count of label by 1\n",
    "                train_counts[label] += 1 \n",
    "                \n",
    "        # count for  for 5 classes(classification) -- class label occurence in validation folder \n",
    "        # -- class labels = 0,1,2,3,4\n",
    "        # -- class labels counts = [0,0,0,0,0] -- indices match with class labels\n",
    "        val_counts = np.zeros(5)\n",
    "        # iterating validation doccument -- path of validation data folders\n",
    "        for doc in val_doc:\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            label = int(doc.strip().split(';')[2])\n",
    "            # select folders from validation data till ablation limit\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            if val_counts[label] < ablation:\n",
    "                # add folder path to validation document\n",
    "                val_data.append(doc)\n",
    "                # increase count of label by 1\n",
    "                val_counts[label] += 1\n",
    "    # when ablation is None pass full training and validation data, and some ablation data is provided\n",
    "    else:\n",
    "        train_data, val_data = train_doc, val_doc\n",
    "        \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e589e04a-9a5a-487e-8b95-8e685df30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and validation data\n",
    "\n",
    "train_doc, val_doc = get_data('../DATA Human Gestures/', ablation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2916370d-5353-4488-a64a-de0044e0b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper-parameters -- experimentation\n",
    "\n",
    "# experiment with the batch size\n",
    "batch_size = 128\n",
    "\n",
    "# augmentation of Data\n",
    "enable_augmentation = False\n",
    "\n",
    "# image sequence ids\n",
    "img_idx = range(5,20,2)\n",
    "\n",
    "# image dimensions\n",
    "dim_x, dim_y = 128, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e863620-d9b2-4266-a603-59cb7d96c1a0",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "* It is there in place so that the generator is always ready to yield a batch once next() is called once it is called at the start of training. \n",
    "* Even after one pass over the data is completed (after the for loop is completed and the batch for the remainder datapoints is yielded), upon the subsequent next() call (at the start of the next epoch), the processing starts from the command 't=np.random.permuatation(folder_list)'. \n",
    "* In this way, the generator requires very less memory while training.\n",
    "\n",
    "\n",
    "* In the generator, \n",
    "    - Preprocess the images as we have images of 2 different dimensions,\n",
    "    - as well as create a batch of video frames.\n",
    "\n",
    "* Experiment with `img_idx`, `y`,`z` and normalization for high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f615a57a-9e94-4312-be7c-dd6770fc616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data generator\n",
    "\n",
    "def generator(\n",
    "    source_path=None, # root directory\n",
    "    folder_list=None, # document\n",
    "    img_idx=None, # list of index of images\n",
    "    batch_size=128, # data size in each batch\n",
    "    is_train = False,\n",
    "    augmentation=False,\n",
    "    debug = False,\n",
    "):\n",
    "    '''\n",
    "    img_idx: \n",
    "        a. experiment with creating a list of image numbers you want to use for a particular video, \n",
    "        b. out of all frame images in each folder -- improve performance\n",
    "    '''    \n",
    "    # samples in image\n",
    "    x = len(img_idx)\n",
    "    # image dimenstion\n",
    "    y, z = dim_x, dim_y\n",
    "    \n",
    "    # continuos flow of data -- for several epochs\n",
    "    while True:\n",
    "        \n",
    "        # doubling the data for augmentation\n",
    "        # randomly select from sequence\n",
    "        if is_train and augmentation:\n",
    "            t = np.concatenate((np.random.permutation(folder_list), np.random.permutation(folder_list)))\n",
    "        else:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            \n",
    "        if (len(t)%batch_size) == 0:\n",
    "            num_batches = int(len(t)/batch_size)\n",
    "        else:\n",
    "            num_batches = len(t)//batch_size + 1\n",
    "                \n",
    "        # 1 sequence = 1 video = 30 image collection\n",
    "        # calculate the number of batches to call in each iterations -- (number of video frames)/(batch size) -- 30/batch_size\n",
    "        num_batches = 6\n",
    "        \n",
    "        # we iterate over the number of batches\n",
    "        for batch in range(num_batches):     \n",
    "            # creating 0 tensors -- empty containers -- load images\n",
    "            # x is the number of images you use for each video = img_idx\n",
    "            # (y,z) is the final size of the input images and  = height,width\n",
    "            # 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            # batch_labels is the one hot ecnoding representation of the output -- 5 classes\n",
    "            batch_labels = np.zeros((batch_size,5)) \n",
    "\n",
    "            # iterate over the batch_size\n",
    "            for folder in range(batch_size): \n",
    "                # read all the images in the video image sequence folder\n",
    "                if debug:\n",
    "                    plt.figure(figsize=(20,5))\n",
    "                #handling remaining datapoints\n",
    "                folder_idx = folder + (batch*batch_size)\n",
    "                if folder_idx >= len(t):\n",
    "                    break\n",
    "                folder_str = t[folder_idx]\n",
    "                imgs = os.listdir(source_path+'/'+ folder_str.split(';')[0]) # read all the images in the folder\n",
    "                # randomly enabling augmentation and augmentation type\n",
    "                aug_type = None\n",
    "                if is_train and augmentation and rn.randint(0,1) == 1:\n",
    "                    #randomly selecting augmentation type\n",
    "                    aug_type = rn.randint(0, 4) \n",
    "                \n",
    "                # Iterate iver the frames/images of a folder to read them in\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    # selectively take images\n",
    "                    image = mpimg.imread(source_path+'/'+ folder_str.strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    # plotting original images for debugging purpose only\n",
    "                    if debug:\n",
    "                        plt.subplot(2, x, idx+1)\n",
    "                        plt.imshow(image.astype('uint8'))\n",
    "        \n",
    "                    # crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    # and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.shape[1] > image.shape[0]:\n",
    "                            diff_px = image.shape[1] - image.shape[0]\n",
    "                            crop_start = diff_px//2\n",
    "                            crop_end = crop_start + image.shape[0]\n",
    "                            image = image[:, crop_start:crop_end]\n",
    "                    elif image.shape[0] > image.shape[1]:\n",
    "                        diff_px = image.shape[0] - image.shape[1]\n",
    "                        crop_start = diff_px//2\n",
    "                        crop_end = crop_start + image.shape[1]\n",
    "                        image = image[:, crop_start:crop_end]\n",
    "                    \n",
    "                    # resize image\n",
    "                    resized_im = image.resize(y,z)\n",
    "                    \n",
    "                    if aug_type is not None:\n",
    "                        if aug_type == 0: # edge Enhancement\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.EDGE_ENHANCE))\n",
    "                        elif aug_type == 1: # adding gaussian blur\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.GaussianBlur(1)))\n",
    "                        elif aug_type == 2: # enchancing image detailing\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.DETAIL))\n",
    "                        elif aug_type == 3: # sharpening image\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.SHARPEN))\n",
    "                        elif aug_type == 4: # Brightness enhancement\n",
    "                            resized_im = np.array(ImageEnhance.Brightness((Image.fromarray(resized_im, 'RGB'))).enhance(1.5))\n",
    "                    \n",
    "                    # plotting rezised images for debugging purpose only\n",
    "                    if debug:\n",
    "                        plt.subplot(2, x, idx+x+1)\n",
    "                        plt.imshow(resized_im)\n",
    "                    \n",
    "                    # normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,0] = (resized_im[:,:,0])/255\n",
    "                    batch_data[folder,idx,:,:,1] = resized_im[:,:,1]/255 \n",
    "                    batch_data[folder,idx,:,:,2] = resized_im[:,:,2]/255 \n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95453bcb-05a1-4c77-915c-11c144c4738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 50\n",
      "# validation sequences = 50\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '../DATA Human Gestures/train' \n",
    "val_path =  '../DATA Human Gestures/val'\n",
    "\n",
    "#multiply number train seq by 2 when using augmentation\n",
    "multiplier = 1\n",
    "if enable_augmentation:\n",
    "    multiplier = 2\n",
    "num_train_sequences = len(train_doc)*multiplier\n",
    "print('# training sequences =', num_train_sequences)\n",
    "\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d7ffb1-8067-4432-9468-431b76fcd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing generative \n",
    "test_gen = generator(\n",
    "    source_path=train_path,folder_list=train_doc,img_idx=img_idx,batch_size=batch_size,\n",
    "    is_train=True,augmentation=True,debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99425596-93e8-46b9-a5d3-2cbbd5e28243",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "resize only works on single-segment arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10096\\1352084701.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10096\\2712273520.py\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m(source_path, folder_list, img_idx, batch_size, is_train, augmentation, debug)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[1;31m# resize image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresized_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0maug_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: resize only works on single-segment arrays"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAAB8CAYAAAB3y2LMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE9ElEQVR4nO3dv2+UBRzH8ffHGhyMgxUhBoh26IKLiQ2Liw5qmeqCKRNDExb5A2ocXP0HdOjQwCK1C7GTSFgcLWyUWDkR4VJiQ3QwDmLJ1+Gemmt715bePXzvnufzSprr/XyeXN95nmufu28VEZhleC57Bay+HJ+lcXyWxvFZGsdnaRyfpSktPkmTklYlNSTNlrUcG14q4+98kkaAn4H3gSawDJyNiNt9X5gNrbK2fKeARkTcjYjHwAIwVdKybEiVFd8x4EHb+WZxmdn/ni/pcdXhsi37d0nngfPF2bdLWg/L9ygiXu10RVnxNYETbeePA2vtN4iIOWAOQJIPMFfXb92uKGu3uwyMSxqTdAiYBpZKWpYNqVK2fBGxIekCcBUYAeYjYqWMZdnwKuVPLU+9Et7tVsbmi/22H+jNiJjodFsf4bC+6RDersr6hcNq6Gl3X97yWRrHZ2kcn6VxfJbG8Vkax2dpHJ+lcXyWxvFZGsdnaRyfpRnIY7vb3wbtt7xU00DG59jqwbtdS+P4LI3jszSOz9I4Pkvj+CyN47M0js/SOD5L4/gsjeOzNI7P0jg+S9PTu1ok3QP+Ap4AGxExIWkU+AZ4A7gHfBwRf/a2mlZF/djyvRcRb7VNIpoFrkfEOHC9OG+2Qxm73SngUvH9JeCjEpZhFdBrfAF8L+lmMWMZ4GhEPAQoTo/0uAyrqF7fyfxORKxJOgJck/TTfu+4bSC41VBPW76IWCtO14ErtP7/xu+SXgMoTte73HcuIia6Ta206jtwfJJelPTS5vfAB8AtWoO/zxU3Owd82+tKWjX1sts9ClyRtPk4X0fEd5KWgUVJM8B94Ezvq2lV5IHgVjYPBLfB4/gsjeOzNI7P0jg+S+P4LI3jszSOz7ZSp//TXY6BHJFmybr11+dDAd7y2VbP8IiXt3y20zPqz1s+S+P4LI3jszSOz9I4Pkvj+CyN47M0js/SOD5L4/gsjeOzNI7P0jg+S+P4LI3jszSOz9I4PkuzZ3yS5iWtS7rVdtmopGuS7hSnL7dd96mkhqRVSR+WteI2/Paz5bsITG67rOPQb0kngWngzeI+X0ka6dvaWqXsGV9E/AD8se3ibkO/p4CFiPgnIn4FGrSmlZrtcNDXfN2Gfh8DHrTdrllcZrZDvz+91ukTnx0/C+WB4HbQLV+3od9N4ETb7Y4Da50ewAPB7aDxdRv6vQRMS3pB0hgwDvzY2ypaZUXErl/AZeAh8C+tLdsM8Aqt33LvFKejbbf/DPgFWAVO7/X4xX3CX5X9utHt5+6B4FY2DwS3weP4LI3jszSOz9I4Pkvj+CyN47M0js/SOD5L4/gszaAMBH8E/F2c2u4OM1zP0+vdrhiIY7sAkm747VV7q9Lz5N2upXF8lmaQ4pvLXoEhUZnnaWBe81n9DNKWz2omPT5Jk8V0g4ak2ez1yVS36RCp8RXTDL4ETgMngbPF1IO6ukiNpkNkb/lOAY2IuBsRj4EFWlMPaqlu0yGy4/OEg71VdjpEdnz7nnBgOwz9c5cd374nHNRYz9MhBlV2fMvAuKQxSYdovYBeSl6nQVPZ6RCp72qJiA1JF4CrwAgwHxErmeuUSdJl4F3gsKQm8DnwBbAoaQa4D5wBiIgVSYvAbWAD+CQinqSs+AH5CIelyd7tWo05Pkvj+CyN47M0js/SOD5L4/gsjeOzNP8BP6skreLg+hUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "next(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33327c-b44e-407e-a60d-e14a50ae03c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffedd68-137f-42fd-9e3d-23822c17653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
