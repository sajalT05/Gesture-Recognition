{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "011c5106-2667-44fd-bedf-5dc931847bbb",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "**Develop a feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.**\n",
    "\n",
    "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "* **Thumbs up**:  Increase the volume\n",
    "* **Thumbs down**: Decrease the volume\n",
    "* **Left swipe**: 'Jump' backwards 10 seconds\n",
    "* **Right swipe**: 'Jump' forward 10 seconds  \n",
    "* **Stop**: Pause the movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9d633-8705-45c4-9b99-3093f468694b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "**Generator**: The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
    "\n",
    "**Model**: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further.\n",
    "\n",
    "**Write up**: This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db413950-cbb4-4f4c-81a7-36a4fb45a954",
   "metadata": {},
   "source": [
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aee66f1-c0d6-4875-93ac-286636a6d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries to get started.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import warnings \n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d10b9b8-4b70-4470-bbc7-da01929ad048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras api\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Bidirectional, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D, GlobalAveragePooling3D, ConvLSTM2D\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efba2e49-d448-45f9-9a13-77fd2ff6a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the random seed so that the results don't vary drastically.\n",
    "\n",
    "np.random.seed(30)\n",
    "random.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b19608-5b5a-4025-8144-e8de06384dc8",
   "metadata": {},
   "source": [
    "Read the folder names for training and validation:\n",
    "* Set the batch_size here, in such a way that you are able to use the GPU in full capacity. \n",
    "* Keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b10d5-4844-45ac-bb11-dde6ed19fce1",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "1. Select training and validation data for ablaion experiment.\n",
    "\n",
    "* Memory and Computation Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7f3f0c-bb9a-4a7f-9195-64dd26dd8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validatiaon data creator with ablation -- select random files for each class\n",
    "'''\n",
    "e.g. : when ablation is 10, then 10 folders for each label will be selected.\n",
    "'''\n",
    "def get_data(path, ablation=None):\n",
    "    \n",
    "    # trainining folder directory document\n",
    "    train_doc = np.random.permutation(open(path+'train.csv').readlines())\n",
    "    # validation folder directory document\n",
    "    val_doc = np.random.permutation(open(path+'val.csv').readlines())\n",
    "    \n",
    "    # count for  for 5 classes(classification) -- class label occurence in train folder \n",
    "    # -- class labels = 0,1,2,3,4\n",
    "    # -- class labels counts = [0,0,0,0,0] -- indices match with class labels\n",
    "    train_counts = np.zeros(5) \n",
    "    \n",
    "    # training data\n",
    "    train_data = []\n",
    "    # valdiatin data\n",
    "    val_data = []\n",
    "    \n",
    "    # when ablation is not None and a limit value is provided, select data till ablation limit\n",
    "    if ablation is not None:\n",
    "        # iterating train doc -- path of training data folders\n",
    "        for doc in train_doc:\n",
    "            # labels of class in folder -- [0,1,2,3,4]\n",
    "            label = int(doc.strip().split(';')[2])\n",
    "            # select folders from training data till ablation limit\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            if train_counts[label] < ablation:\n",
    "                # add folder path to training document\n",
    "                train_data.append(doc)\n",
    "                # increase count of label by 1\n",
    "                train_counts[label] += 1 \n",
    "                \n",
    "        # count for  for 5 classes(classification) -- class label occurence in validation folder \n",
    "        # -- class labels = 0,1,2,3,4\n",
    "        # -- class labels counts = [0,0,0,0,0] -- indices match with class labels\n",
    "        val_counts = np.zeros(5)\n",
    "        # iterating validation doccument -- path of validation data folders\n",
    "        for doc in val_doc:\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            label = int(doc.strip().split(';')[2])\n",
    "            # select folders from validation data till ablation limit\n",
    "            # if count of label is less than ablatin limit, then only select value\n",
    "            if val_counts[label] < ablation:\n",
    "                # add folder path to validation document\n",
    "                val_data.append(doc)\n",
    "                # increase count of label by 1\n",
    "                val_counts[label] += 1\n",
    "    # when ablation is None pass full training and validation data, and some ablation data is provided\n",
    "    else:\n",
    "        train_data, val_data = train_doc, val_doc\n",
    "        \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e589e04a-9a5a-487e-8b95-8e685df30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and validation data\n",
    "\n",
    "train_doc, val_doc = get_data('../DATA Human Gestures/', ablation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2916370d-5353-4488-a64a-de0044e0b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper-parameters -- experimentation\n",
    "\n",
    "# experiment with the batch size\n",
    "batch_size = 128\n",
    "\n",
    "# augmentation of Data\n",
    "enable_augmentation = False\n",
    "\n",
    "# image sequence ids -- selecting alternate frames\n",
    "seq_idx = list(range(5,25,3))\n",
    "\n",
    "# image dimensions\n",
    "dim_x, dim_y = 128, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e863620-d9b2-4266-a603-59cb7d96c1a0",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "* It is there in place so that the generator is always ready to yield a batch once next() is called once it is called at the start of training. \n",
    "* Even after one pass over the data is completed (after the for loop is completed and the batch for the remainder datapoints is yielded), upon the subsequent next() call (at the start of the next epoch), the processing starts from the command 't=np.random.permuatation(folder_list)'. \n",
    "* In this way, the generator requires very less memory while training.\n",
    "\n",
    "\n",
    "* In the generator, \n",
    "    - Preprocess the images as we have images of 2 different dimensions,\n",
    "    - as well as create a batch of video frames.\n",
    "\n",
    "* Experiment with `img_idx`, `y`,`z` and normalization for high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f615a57a-9e94-4312-be7c-dd6770fc616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data generator\n",
    "\n",
    "def generator(\n",
    "    source_path=None, # root directory\n",
    "    folder_list=None, # document\n",
    "    img_idx=None, # list of index of images\n",
    "    batch_size=128, # data size in each batch\n",
    "    is_train = False,\n",
    "    augmentation=False,\n",
    "    debug = False,\n",
    "):\n",
    "    '''\n",
    "    img_idx: \n",
    "        a. experiment with creating a list of image numbers you want to use for a particular video, \n",
    "        b. out of all frame images in each folder -- improve performance\n",
    "    '''    \n",
    "    # samples in image\n",
    "    x = len(img_idx)\n",
    "    # image dimenstion\n",
    "    y, z = dim_x, dim_y\n",
    "    \n",
    "    # continuos flow of data -- for several epochs\n",
    "    while True:\n",
    "        \n",
    "        # doubling the data for augmentation\n",
    "        # randomly select from sequence\n",
    "        if is_train and augmentation:\n",
    "            t = np.concatenate((np.random.permutation(folder_list), np.random.permutation(folder_list)))\n",
    "        else:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            \n",
    "        # 1 sequence = 1 video = 30 image collection\n",
    "        # calculate the number of batches to call in each iterations -- (number of video frames)/(batch size) -- 30/batch_size\n",
    "        if (len(t)%batch_size) == 0:\n",
    "            num_batches = int(len(t)/batch_size)\n",
    "        else:\n",
    "            num_batches = len(t)//batch_size + 1                \n",
    "        \n",
    "        # we iterate over the number of batches\n",
    "        for batch in range(num_batches):     \n",
    "            # creating 0 tensors -- empty containers -- load images\n",
    "            # x is the number of images you use for each video = img_idx\n",
    "            # (y,z) is the final size of the input images and  = height,width\n",
    "            # 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            # batch_labels is the one hot ecnoding representation of the output -- 5 classes\n",
    "            batch_labels = np.zeros((batch_size,5)) \n",
    "\n",
    "            # iterate over the batch_size\n",
    "            for folder in range(batch_size): \n",
    "                # read all the images in the video image sequence folder\n",
    "                if debug:\n",
    "                    plt.figure(figsize=(20,5))\n",
    "                #handling remaining datapoints\n",
    "                folder_idx = folder + (batch*batch_size)\n",
    "                if folder_idx >= len(t):\n",
    "                    break\n",
    "                folder_str = t[folder_idx]\n",
    "                imgs = os.listdir(source_path+'/'+ folder_str.split(';')[0]) # read all the images in the folder\n",
    "                \n",
    "                # randomly enabling augmentation and augmentation type\n",
    "                aug_type = None\n",
    "                if is_train and augmentation and rn.randint(0,1) == 1:\n",
    "                    #randomly selecting augmentation type\n",
    "                    aug_type = rn.randint(0, 4) \n",
    "                \n",
    "                # Iterate iver the frames/images of a folder to read them in\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    # selectively take images\n",
    "                    image = mpimg.imread(source_path+'/'+ folder_str.strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "        \n",
    "                    # crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    # and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.shape[1] > image.shape[0]:\n",
    "                            diff_px = image.shape[1] - image.shape[0]\n",
    "                            crop_start = diff_px//2\n",
    "                            crop_end = crop_start + image.shape[0]\n",
    "                            image = image[:, crop_start:crop_end]\n",
    "                    elif image.shape[0] > image.shape[1]:\n",
    "                        diff_px = image.shape[0] - image.shape[1]\n",
    "                        crop_start = diff_px//2\n",
    "                        crop_end = crop_start + image.shape[1]\n",
    "                        image = image[:, crop_start:crop_end]\n",
    "                    \n",
    "                    # resize image\n",
    "                    resized_im = resize(image, (y,z))\n",
    "                    \n",
    "                    if aug_type is not None:\n",
    "                        if aug_type == 0: # edge Enhancement\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.EDGE_ENHANCE))\n",
    "                        elif aug_type == 1: # adding gaussian blur\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.GaussianBlur(1)))\n",
    "                        elif aug_type == 2: # enchancing image detailing\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.DETAIL))\n",
    "                        elif aug_type == 3: # sharpening image\n",
    "                            resized_im = np.array(Image.fromarray(resized_im, 'RGB').filter(ImageFilter.SHARPEN))\n",
    "                        elif aug_type == 4: # Brightness enhancement\n",
    "                            resized_im = np.array(ImageEnhance.Brightness((Image.fromarray(resized_im, 'RGB'))).enhance(1.5))\n",
    "                    \n",
    "                    # plotting original images for debugging purpose only\n",
    "                    if debug:\n",
    "                        plt.subplot(2, x, idx+1)\n",
    "                        plt.imshow(image)\n",
    "                    \n",
    "                    # normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,0] = resized_im[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = resized_im[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = resized_im[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95453bcb-05a1-4c77-915c-11c144c4738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 50\n",
      "# validation sequences = 50\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = '../DATA Human Gestures/train' \n",
    "val_path =  '../DATA Human Gestures/val'\n",
    "\n",
    "#multiply number train seq by 2 when using augmentation\n",
    "multiplier = 1\n",
    "if enable_augmentation:\n",
    "    multiplier = 2\n",
    "\n",
    "num_train_sequences = len(train_doc)*multiplier\n",
    "print('# training sequences =', num_train_sequences)\n",
    "\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d33327c-b44e-407e-a60d-e14a50ae03c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14636\\3179498204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14636\\1688320260.py\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m(source_path, folder_list, img_idx, batch_size, is_train, augmentation, debug)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# read all the images in the video image sequence folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[1;31m#handling remaining datapoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mfolder_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# testing generative \n",
    "\n",
    "test_gen = generator(\n",
    "    source_path=train_path,folder_list=train_doc,img_idx=seq_idx,batch_size=1,\n",
    "    is_train = True, augmentation = True, debug = True\n",
    ")\n",
    "\n",
    "d = next(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27817b99-ec38-41f5-b4ab-52cfc26b22c3",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- Make the model using different functionalities that Keras provides. \n",
    "- Use Conv3D and MaxPooling3D and not Conv2D and Maxpooling2D for a 3D convolution model. \n",
    "\n",
    "- Use TimeDistributed while building a Conv2D + RNN model. \n",
    "- Also remember that the last layer is the softmax. \n",
    "\n",
    "Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ffb2b-a8fe-48ac-8ba5-3cf398d6a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input shape for the st layer\n",
    "input_shape = (len(seq_idx), dim_x, dim_y, 3)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1b45c-9655-4fe3-9d40-ba7144e63b3e",
   "metadata": {},
   "source": [
    "**model 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6be2860a-2ed0-4b5f-836d-64dccb4c9276",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv3d_2. Consider increasing the input size. Received input shape [None, 1, 62, 62, 64] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10876\\608223709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    348\u001b[0m                 \u001b[1;34m\"One of the dimensions in the output is <= 0 \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;34mf\"due to downsampling in {self.name}. Consider \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv3d_2. Consider increasing the input size. Received input shape [None, 1, 62, 62, 64] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv3D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling3D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c88adc-4dea-4031-b942-596559dab10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
